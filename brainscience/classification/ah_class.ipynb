import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from collections import Counter

from sklearn import model_selection
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold,StratifiedKFold
 
from sklearn import tree
from sklearn import metrics

from sklearn.metrics import mean_absolute_error


from imblearn.under_sampling import TomekLinks


#from sklearn.metrics import classification_report, accuracy_score 
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import xgboost as xgb
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, confusion_matrix

from imblearn.metrics import geometric_mean_score

ds = pd.read_csv("ahc.csv")
ds.head()

# divide into the training and test data sets.
tr = ds.columns[:-1]


#X_train,X_test,y_train,y_test = model_selection.train_test_split(ds[tr], ds.churn, test_size=0.25,random_state=67)


kf = KFold(n_splits=10,shuffle=False)

def Catclass(clas, X, target):
    acc = []
    rec = []
    f1 = []
    gm = []
    mae = []
    for train, test in kf.split(X,target): 
        X_train = X.loc[train]
        y_train = target.loc[train]    
        
    
        
        #UNDERSAMPLING
        
        #TomekLinks
        #utl = TomekLinks() 
        #usa_X,usa_y = utl.fit_sample(X_train, y_train)
        
        
      
        # The proportion of categories before resampling
        print(y_train.value_counts()/len(y_train))
        # The proportion of categories after resampling
        #print('After resampling:')
        #print(pd.Series(usa_y).value_counts()/len(usa_y))
       
        
        
        X_test = X.loc[test]
        y_test = target.loc[test]

        #accuracy of model prediction and other metrics
       
        #clas.fit(usa_X,usa_y) # undersampling 
        
        clas.fit(X_train, y_train)
        pred = clas.predict(X_test)
        print(metrics.classification_report(y_test, pred))
        acc.append(metrics.accuracy_score(y_test, pred))
        rec.append(metrics.recall_score(y_test, pred))
        f1.append(metrics.f1_score(y_test, pred)) 
        gm.append(geometric_mean_score(y_test, pred))
        #mae.append(mean_absolute_error(y_test, pred))
    print('Modelprediction-Accuracy: %.4f; Standard Deviation: %.4f' % (np.mean(acc), np.std(acc)))
    print('Recall: %.4f; Standard Deviation: %.4f' % (np.mean(rec), np.std(rec)))
    print('F1-score: %.4f; Standard Deviation: %.4f' % (np.mean(f1), np.std(f1)))
    print('G-means: %.4f; Standard Deviation: %.4f' % (np.mean(gm), np.std(gm))) 

    #print('MAE: %.4f; Standard Deviation: %.4f' % (np.mean(mae), np.std(mae)))
#     results = cross_val_score(clas, X, target, cv=kf, scoring='neg_mean_absolute_error')
#     print(results)
#     print("Accuracy: %.4f (%.4f)" % (results.mean(), results.std()))
    

    

Catclass(RandomForestClassifier(random_state = 67), ds[tr],ds.symptom)

Catclass(xgb.XGBClassifier(n_estimators=500, learning_rate=0.1, random_state = 67), ds[tr],ds.symptom)

Catclass(AdaBoostClassifier(random_state = 67), ds[tr],ds.symptom)

Catclass(svm.SVC(probability=True,random_state = 67), ds[tr],ds.symptom)
